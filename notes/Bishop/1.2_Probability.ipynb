{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9520939-fbda-400b-bf22-ef634bf5a112",
   "metadata": {},
   "source": [
    "## Probability Theory\n",
    "\n",
    "Probability theory provides a consistent framework for the quantification and manipulation of uncertainty.\n",
    "\n",
    "Consider two random variables $X$ and $Y$. Suppose that $X$ can take any of the values $x_i$ where $i=1,...,M$ and $Y$ can take the values $y_j$ where $j=1,...,L$. Consider a total of $N$ trials in which we sample both $X$ and $Y$, and let the number of such trials in which $X=x_i$ and $Y=y_j$ be $n_{ij}$. Also let the number of trials in which $X$ takes the value $x_i$ (irrespective of the value $Y$ takes) be denoted by $c_i$, and similarly let the number of trials in which $Y$ takes the value $y_j$ be denoted by $r_j$.\n",
    "\n",
    "The probability that $X$ will take the value $x_i$ and $Y$ will take the value $y_j$ is written $p(X=x_i, Y=y_j)$ and is called the *joint* probability of $X=x_i$ and $Y=y_j$, and can be expressed as\n",
    "\n",
    "$$\n",
    "p(X=x_i, Y=y_j)=\\frac{n_{ij}}{N}\n",
    "$$\n",
    "where we implicitly consider the limit $N\\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daae78d-6dbd-43d5-b253-672165b8e742",
   "metadata": {},
   "source": [
    "The probability that $X$ takes the value $x_i$ irrespective of the value of $Y$ is written as $p(X=x_i)$ and is given by\n",
    "\n",
    "$$\n",
    "p(X=x_i)=\\frac{c_i}{N}\n",
    "$$\n",
    "Since this is independent of the value of $Y$, we have $c_i=\\sum_j n_{ij}$ and\n",
    "\n",
    "$$\n",
    "p(X=x_i)=\\sum^L_{j=1}p(X=x_i, Y=y_j)\n",
    "$$\n",
    "which is the *sum rule* of probability. $p(X=x_i)$ is sometimes called the *marginal* probability, because it is obtained by marginalising, or summing out, the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3dc769-7e1f-4c94-8af3-04ad055bf8fc",
   "metadata": {},
   "source": [
    "If we consider only the instances for which $X=x_i$, then the fraction of such instances for which $Y=y_j$ is written $p(Y=y_j\\vert X=x_i)$ and is called the *conditional* probability of $Y=j_j$ given $X=x_i$, and is given by\n",
    "$$\n",
    "p(Y=y_j\\vert X=x_i)=\\frac{n_{ij}}{c_i}\n",
    "$$\n",
    "We can derive the following relationship\n",
    "$$\n",
    "p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i}\\cdot\\frac{c_i}{N} \\\\\n",
    "=p(Y=y_j\\vert X=x_i)p(X=x_i)\n",
    "$$\n",
    "which is the *product rule* of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ae066-df45-4ba1-b584-b9bbe78ea1e6",
   "metadata": {},
   "source": [
    "### The Rules of Probability\n",
    "\n",
    "**sum rule**:\n",
    "$$\n",
    "p(X) = \\sum^{}_Y p(X,Y)\n",
    "$$\n",
    "**product rule**\n",
    "$$\n",
    "p(X,Y) = p(Y\\vert X)p(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3543be1-ae1f-49f7-8557-ad9778ed6546",
   "metadata": {},
   "source": [
    "From the product rule, and the symmetry property $p(X,Y)=p(Y,X)$, we obtain the following relationship between conditional probabilities\n",
    "\n",
    "$$\n",
    "p(Y\\vert X)=\\frac{p(X\\vert Y)p(Y)}{p(X)}\n",
    "$$\n",
    "which is called *Bayes' theorem*. Using the sum rule, the denominator in Bayes' theorem can be expressed in terms of quantities appearing in the numerator\n",
    "\n",
    "$$\n",
    "p(X)=\\sum^{}_Y p(X\\vert Y)p(Y)\n",
    "$$\n",
    "We can view the denominator in Bayes' theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side in the definition of Bayes' theorem over all values of $Y$ equals one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612b936-98a5-49e6-8811-032f7bac5f04",
   "metadata": {},
   "source": [
    "### Probability densities\n",
    "\n",
    "We also want to consider probabilities with respect to continuous variables. If the probability of a real-values variable $x$ falling in the interval $(x, x+\\delta x)$ is given by $p(x)\\delta x$ for $\\delta x \\rightarrow 0$, then $p(x)$ is called the *probability density* over $x$. The probability that $x$ will lie in an interval $(a,b)$ is then given by\n",
    "\n",
    "$$\n",
    "p(x\\in (a,b)) = \\int_a^b p(x)~\\mathrm{d}x\n",
    "$$\n",
    "Because probabilities are non negative and because the value of $x$ must lie somewhere on the real axis, the probability density $p(x)$ must satisfy two conditions\n",
    "\n",
    "$$\n",
    "~~~~~~~~~~~~~p(x) \\geqslant 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int_\\infty^\\infty p(x)~\\mathrm{d}x = 1\n",
    "$$\n",
    "\n",
    "Under a nonlinear change of variable, a probability density transforms differently from a simple function due to the Jacobian factor. Consider a change of variables $x=g(y)$, then $f(x)$ becomes $\\tilde{f}(y) = f(g(y))$. Consider a probability density $p_x(x)$ that corresponds to a density $p_y(y)$ with respect to the new variable $y$. Observations falling in the range $(x, x+\\delta x)$ will, for small values of $\\delta x$ be transformed into the range $(y,y+\\delta y)$ where $p_x(x)\\delta x \\simeq p_y(y)\\delta y$, hence\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_y(y) & = p_x(x)\\bigg\\vert\\frac{\\mathrm{d}x}{\\mathrm{d}y}\\bigg\\vert\\\\\n",
    "& = p_x(g(y))\\vert g'(y)\\vert\n",
    "\\end{aligned}\n",
    "$$\n",
    "One consequence of this property is the concept of the maximum of a probability density is dependent on the choice of variable\n",
    "\n",
    "The probability that $x$ lies in the interval $(-\\infty, z)$ is given by the *cumulative distribution function* defined by\n",
    "\n",
    "$$\n",
    "P(z) = \\int_{-\\infty}^z p(x)~\\mathrm{d}x\n",
    "$$\n",
    "which satisfies $P'(x)=p(x)$.\n",
    "\n",
    "If we have several continuous variables $x_1,...,x_D$, denoted collectively by the vector $\\mathbf{x}$, then we can define a joint probability density $p(\\mathbf{x})=p(x_1,...,x_D)$ such that the probability of $\\mathbf{x}$ falling in an infinitesimal volume $\\delta\\mathbf{x}$ containing the point $\\mathbf{x}$ is given by $p(\\mathbf{x})\\delta\\mathbf{x}$. This multivariate probability density must satisfy\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{x} & \\geqslant 0 \\\\\n",
    "\\int_{}^{}p(\\mathbf{x})~\\mathrm{d}x & = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "in which the integral is taken over the whole of $\\mathbf{x}$ space. We can also consider ajoint probability distributions over a combination of discrete and continuous variables. If $x$ is a discrete variable, then $p(x)$ is called a *probability mass function*.\n",
    "\n",
    "The sum and product rules of probability, as well as Bayes' theorem, apply equally to the case of probability densities, or to combinations of discrete and continous variables. If $x$ and $y$ are two real variables, then the sum and product rules take the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x) & = \\int_{}^{}p(x,y)~\\mathrm{d}y \\\\\n",
    "p(x,y) & = p(y\\vert x)p(x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "and Bayes' theorem is given by\n",
    "\n",
    "$$\n",
    "p(y\\vert x) = \\frac{p(x\\vert y)(p(y)}{\\int_{}^{}p(x,y)~\\mathrm{d}y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35219c-d9f6-408f-ba2f-f5403dc51116",
   "metadata": {},
   "source": [
    "### Expectations and covariances\n",
    "\n",
    "The average value of some function $f(x)$ under a probability distirbution $p(x)$ is called the *expectation* of $f(x))$ and is denoted by $\\mathbb{E}[f]$.\n",
    "\n",
    "For a discrete distribution, the expectation is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[f] = \\sum^{}_x p(x)f(x)\n",
    "$$\n",
    "so the average is weighted by the relatiive probabilities of the different values of $x$.\n",
    "\n",
    "For continuous variables, expectations are expressed in terms of an integration with respect to the corresponding probability density\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[f] = \\int^{}_{}p(x)f(x)~\\mathrm{d}x\n",
    "$$\n",
    "\n",
    "In either case, given a finite number $N$ of points drawn from the probability distribution or probability density, the expectation can be approximated as a finite sum over these points\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[f]\\simeq\\frac{1}{N}\\sum^{N}_{n=1}f(x_n)\n",
    "$$\n",
    "\n",
    "Sometimes we consider expectations of functions of several variables, in which case we can use a subscript to indicate which variable is being averaged over, so that for instance\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_x[f(x,y)]\n",
    "$$\n",
    "\n",
    "denotes the average of the function $f(x,y)$ with respect to the distribution of $x$. Note that $\\mathbb{E}_x[f(x,y)]$ will be a function of $y$.\n",
    "\n",
    "We can also consider a *conditional expectation* with respect to a conditional distribution, so that\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_x[f\\vert t]=\\sum^{}_x p(x\\vert y)f(x)\n",
    "$$\n",
    "\n",
    "with an analogous definition for continuous variables.\n",
    "\n",
    "The *variance* of $f(x)$ is defined by\n",
    "\n",
    "$$\n",
    "\\mathrm{var}[f] = \\mathbb{E}\\big[(f(x) - \\mathbb{E}[f(x)])^2\\big]\n",
    "$$\n",
    "\n",
    "and provides a measure of of how much variability there is in $f(x)$ around its mean value $\\mathbb{E}[f(x)]$. Expanding out the square, we see that variance can also be written in terms of the expectations of $f(x)$ and $f(x)^2$\n",
    "\n",
    "$$\n",
    "\\mathrm{var}[f] = \\mathbb{E}[f(x)^2] - \\mathbb{E}[f(x)]^2\n",
    "$$\n",
    "\n",
    "We can consider the variance of the variable $x$ itself, which is given by\n",
    "\n",
    "$$\n",
    "\\mathrm{var}[x] = \\mathrm{E}[x^2] - \\mathrm{E}[x]^2\n",
    "$$\n",
    "\n",
    "For two random variables $x$ and $y$, the *covariance* is defined by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{cov}[x,y] & = \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n",
    "& = \\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n",
    "\\end{aligned}\n",
    "$$\n",
    "which expresses the extent to which $x$ and $y$ vary together. If $x$ and $y$ are independent then their covariance vanishes.\n",
    "\n",
    "In the case of two vector of random variables $\\mathbf{x}$ and $\\mathbf{y}$, the covariance is a matrix\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{cov}[\\mathbf{x},\\mathbf{y}] & = \\mathbb{E}_{\\mathbf{x},\\mathbf{y}}\\big[ \\{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^\\mathrm{T} - \\mathbb{E}[\\mathbf{y}^\\mathrm{T}]\\}  \\big] \\\\\n",
    "& = \\mathbb{E}_{\\mathbf{x}, \\mathbf{y}}[\\mathbf{x}\\mathbf{y}^\\mathrm{T}] - \\mathbb{E}[\\mathbf{x}\\mathbb{E}[\\mathbf{y}^\\mathrm{T}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we consider the covariance of the components of a vector $\\mathbf{x}$ with each other, then we can use a simpler notation $\\mathrm{cov}[\\mathbf{x}]\\equiv\\mathrm{cov}[\\mathbf{x},\\mathbf{x}]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
