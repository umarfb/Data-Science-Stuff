{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17f86d6-6582-40ca-8ea3-903b4f3673a7",
   "metadata": {},
   "source": [
    "## 1.2.5 Bayesian curve fitting\n",
    "\n",
    "The goal in curve fitting is to be able to make predictions for the target variable $t$ given some new value of the input variable $x$ on the basis of a set of training data comprising $N$ input values $\\mathsf{x}=(x_1,...,x_N)^{\\mathrm{T}}$ and their corresponding target values $\\mathsf{t}=(t_1,...,t_N)^{\\mathrm{T}}$. We can express our uncertainty over the value of the target variable using a probability distribution. Assume that given the value of $x$, the corresponding value of $t$ has a Gaussian distribution with a mean equal to the value $y(x,\\mathbf{W})$ of the polynomial curve. We have\n",
    "\n",
    "$$\n",
    "p(t\\vert  x,\\mathbf{w},\\beta) = \\mathcal{N}(t\\vert y(x,\\mathbf{w}),\\beta^{-1})\n",
    "$$\n",
    "where we define a precision parameter $\\beta$ corresponding to the inverse variance of the distribution.\n",
    "\n",
    "We use the training data $\\{\\mathsf{x},\\mathsf{t}\\}$ to determine the values of the unknown parameters $\\mathbf{w}$ and $\\beta$ by the maximum likelihood. If the data is assumed to be i.i.d, then the likelihood function is\n",
    "\n",
    "$$\n",
    "p(\\mathsf{t}\\vert\\mathsf{x},\\mathbf{w},\\beta)=\\prod^N_{n=1}\\mathcal{N}(t_n\\vert y(x_n,\\mathbf{w}),\\beta^{-1})\n",
    "$$\n",
    "\n",
    "It is convenient to maximise the logarithm of the likelihood function. Substituting in the form of the Gaussian distribution\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\big(x\\vert\\mu,\\sigma^2\\big) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Bigg\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\Bigg\\}\n",
    "$$\n",
    "we obtain the log likelihood function\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathsf{t}\\vert\\mathsf{x},\\mathbf{w},\\beta)=-\\frac{\\beta}{2}\\sum^N_{n=1}\\{y(x_n, \\mathbf{w})-t_n\\}^2 + \\frac{N}{2}\\ln\\beta -\\frac{N}{2}\\ln(2\\pi)\n",
    "$$\n",
    "\n",
    "Consider the determination of the maximum likelihood solution for the polynomial coefficients denoted by $\\mathbf{w}_{\\mathrm{ML}}$. These are determined by maximising the log likelihood with respect to $\\mathbf{w}$. For this purpose, we can omit the last two terms on the right-hand side as they do not depend on $\\mathbf{w}$. The scaling of the log likelihood by a positive constant coefficient does not change the location of the maximum with respect to $\\mathbf{w}$, so we can replace the coefficient $\\beta/2$ with $1/2$. Instead of maximising the log likelihood, we can equivalently minimize the negative log likelihood. We see that maximising likelihood is equivalent when, determing $\\mathbf{w}$ is concerned, to minimising the *sum-of-squares error function*. The sum-of-squares error function has arisen as a consequence of maximising likelihood under the assumption of a Gaussian noise distribution.\n",
    "\n",
    "We can use the maximimum likelihood to determine the precision parameter $\\beta$ of the Gaussian conditional distribution. Maximising the log likelihood with respect to $\\beta$ gives\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\beta_{\\mathrm{ML}}}=\\frac{1}{N}\\sum^N_{n=1}\\{ y(x_n,\\mathbf{w}_{\\mathrm{ML}})-t_N \\}^2\n",
    "$$\n",
    "\n",
    "We can first determine the parameter vector $\\mathbf{w}_{\\mathrm{ML}}$ governing the mean and then use this to find the precision $\\beta_{\\mathrm{ML}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0602fa-9241-4933-9132-b4f6ae1688f1",
   "metadata": {},
   "source": [
    "Having determined the parameters $\\mathbf{w}$ and $\\beta$, we can make predictions for new values of $x$. We now have a probabilistic model, expressed in terms of *predictive distribution* that gives the probability distribution over $t$, rather than simply a point estimate, and is obtained by substituing the maximum likelihood parameters to give\n",
    "\n",
    "$$\n",
    "p(t\\vert x, \\mathbf{w}_{\\mathrm{ML}},\\beta_{\\mathrm{ML}}) = \\mathcal{N}(t\\vert y(x, \\mathbf{w}_{\\mathrm{ML}}), \\beta^{-1}_{\\mathrm{ML}})\n",
    "$$\n",
    "\n",
    "We can take a step towards a more Bayesian approach and introduce a prior distribution over $\\mathbf{w}$. Consider a Gaussian distribution of the form\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}\\vert\\alpha) = \\mathcal{N}(\\mathbf{w}\\vert\\mathbf{0},\\alpha^{-1}\\mathbf{I})=\\Big(\\frac{\\alpha}{2\\pi}\\Big)^{(M+1)/2}\\exp\\Big\\{ -\\frac{\\alpha}{2}\\mathbf{w}^{\\mathrm{T}}\\mathbf{w} \\Big\\}\n",
    "$$\n",
    "where $\\alpha$ is the precision of the distribution, and $M+1$ is the total number of elements in the vector $\\mathbf{w}$ for an $M^{\\mathrm{th}}$ order polynomial. Variables such as $\\alpha$, which control the distribution of model parameters are called *hyperparameters*. Using Bayes' theorem, the posterior distribution for $\\mathbf{w}$ is\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}\\vert\\mathsf{x},\\mathsf{t},\\alpha,\\beta) \\propto p(\\mathsf{t}\\vert\\mathsf{x},\\mathbf{w},\\beta)p(\\mathbf{w}\\vert\\alpha)\n",
    "$$\n",
    "We can determine $\\mathbf{w}$ by finding the most probable value of $\\mathbf{w}$ given the data, in other words by maximising the posterior distribution. This technique is called *maximum posterior* or *MAP*. Taking the negative logarithm and combining with the log likelihood and the above Gaussian, the maximum posterior is given my the minimum of\n",
    "\n",
    "$$\n",
    "\\frac{\\beta}{2}\\sum^N_{n=1}\\{ y(x_n,\\mathbf{w}) -t_n\\}^2 + \\frac{\\alpha}{2}\\mathbf{w}^{\\mathrm{T}}\\mathbf{w}\n",
    "$$\n",
    "Maximising the posterior distribution is equivalent to minimsing the regularised sum-of-squares error function with a regularisation parameter given by $\\lambda=\\alpha/\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956e712-5d3f-494e-b6d7-7c9d47485e98",
   "metadata": {},
   "source": [
    "### A Bayesian approach\n",
    "\n",
    "In a fully Bayesian approach, we should consistently apply the sum and product rules of probability which requires that we integrate over all values of $\\mathbf{w}$.\n",
    "\n",
    "In the curve fitting problem, we are given the training data $\\mathsf{x}$ and $\\mathsf{t}$, along with a new test point $x$, and our goal is to predict the value of $t$. We wish to evaluate the predictive distribution $p(t\\vert x,\\mathsf{x}, \\mathsf{t})$. We shall assume that the parameters $\\alpha$ and $\\beta$ are fixed and known in advance. A Bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability, which allow the predictive distribution to be written in the form\n",
    "\n",
    "$$\n",
    "p(t\\vert x,\\mathsf{x}\\mathsf{t}) = \\int_{}^{}p(t\\vert x, \\mathbf{w})p(\\mathbf{w}\\vert\\mathsf{x},\\mathsf{t})~\\mathrm{d}\\mathbf{w}\n",
    "$$\n",
    "Here $p(\\mathbf{w}\\vert\\mathsf{x},\\mathsf{t})$ is the posterior distribution over parameters, and can be found by normalising the right-hand side of the posterior distribution for $\\mathbf{w}$. The integration on the right-hand side of the predictive distribution above can be performed analytically with the result that the predictive distribution is given by a Gaussian of the form\n",
    "\n",
    "$$\n",
    "p(t\\vert x,\\mathsf{x}\\mathsf{t})=\\mathcal{N}\\big( t\\vert m(x),s^2(x) \\big)\n",
    "$$\n",
    "where the mean and variance are given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m(x) & = \\beta\\phi(x)^\\mathrm{T}\\mathbf{S}\\sum_{n=1}^{N}\\phi(x_n)t_n\\\\\n",
    "s^2(x) & = \\beta^{-1} + \\phi(x)^{\\mathrm{T}}\\mathbf{S}\\phi(x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here the matrix $\\mathbf{S}$ is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{S}^{-1}=\\alpha\\mathbf{I}+\\beta\\sum^{N}_{n=1}\\phi(x_n)\\phi(x)^\\mathrm{T}\n",
    "$$\n",
    "where $\\mathbf{I}$ is the unit matrix, and we have defined the vector $\\phi(x)$ with elements $\\phi_i(x)=x^i$ for $i=0,...,M$. \n",
    "\n",
    "The variance, as well as the mean, of the predictive distribution is dependent on $x$. The first term in the variance represents the uncertainty in the predicted value of $t$ due to the noise on the target variable and was expressed already in the maximum likelihood predictive distribution through $\\beta^{-1}_{\\mathrm{ML}}$. The seconds term arises from the uncertainty in the parameters $\\mathbf{w}$ and is a consequence of the Bayesian treatment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
